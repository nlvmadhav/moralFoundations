{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Foundation'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Tweet Text</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Foundation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EVM</td>\n",
       "      <td>#EVMs are easy to manipulate . This was demons...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAA</td>\n",
       "      <td>Congratulations Aa gya  #CAA</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAA</td>\n",
       "      <td>My dear Indian muslims we are 35 crores of pop...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAA</td>\n",
       "      <td>This guy was part of anti Hindu protests in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAA</td>\n",
       "      <td>Woman protester breaks down while speaking to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Corpus                                         Tweet Text  Stance Foundation\n",
       "0    EVM  #EVMs are easy to manipulate . This was demons...       1       None\n",
       "1    CAA                       Congratulations Aa gya  #CAA       1       None\n",
       "2    CAA  My dear Indian muslims we are 35 crores of pop...      -1       None\n",
       "3    CAA  This guy was part of anti Hindu protests in th...       1       None\n",
       "4    CAA  Woman protester breaks down while speaking to ...       0       None"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for each class:\n",
      "Corpus\n",
      "CAA                     207\n",
      "UCC                     190\n",
      "EVM                     181\n",
      "LGBTQ                   172\n",
      "Animal Sacrifices       166\n",
      "Anti Conversion Laws    162\n",
      "Brain Drain             148\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of rows: 1226\n"
     ]
    }
   ],
   "source": [
    "class_counts = data.iloc[:, 0].value_counts()\n",
    "\n",
    "print(\"Number of rows for each class:\")\n",
    "print(class_counts)\n",
    "\n",
    "total_rows = data.shape[0]\n",
    "print(\"\\nTotal number of rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Corpus', data=data)\n",
    "plt.title('Distribution of Classes', fontsize=16)\n",
    "plt.xlabel('Corpus', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'@[\\w_]+', '', tweet)  \n",
    "    tweet = re.sub(r'#\\w+', '', tweet)     \n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)  \n",
    "    tweet = re.sub(r'[^A-Za-z0-9\\s]+', '', tweet) \n",
    "    tweet = tweet.strip().lower()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['cleaned_tweet'] = data['Tweet Text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Tweet Text</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EVM</td>\n",
       "      <td>#EVMs are easy to manipulate . This was demons...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>are easy to manipulate  this was demonstrated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAA</td>\n",
       "      <td>Congratulations Aa gya  #CAA</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>congratulations aa gya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAA</td>\n",
       "      <td>My dear Indian muslims we are 35 crores of pop...</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>my dear indian muslims we are 35 crores of pop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAA</td>\n",
       "      <td>This guy was part of anti Hindu protests in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>this guy was part of anti hindu protests in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAA</td>\n",
       "      <td>Woman protester breaks down while speaking to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>woman protester breaks down while speaking to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Corpus                                         Tweet Text  Stance  \\\n",
       "0    EVM  #EVMs are easy to manipulate . This was demons...       1   \n",
       "1    CAA                       Congratulations Aa gya  #CAA       1   \n",
       "2    CAA  My dear Indian muslims we are 35 crores of pop...      -1   \n",
       "3    CAA  This guy was part of anti Hindu protests in th...       1   \n",
       "4    CAA  Woman protester breaks down while speaking to ...       0   \n",
       "\n",
       "  Foundation                                      cleaned_tweet  \n",
       "0       None  are easy to manipulate  this was demonstrated ...  \n",
       "1       None                             congratulations aa gya  \n",
       "2       None  my dear indian muslims we are 35 crores of pop...  \n",
       "3       None  this guy was part of anti hindu protests in th...  \n",
       "4       None  woman protester breaks down while speaking to ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot predictions for initial prdictions\n",
    "####     -Using hugging face bart-large-mnli model for making predictions\n",
    "####     -Classifying a total of 15% of the total tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier\n",
    "classifier = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data seg\n",
    "zero_samples = data.sample(frac=0.15, random_state=42)\n",
    "moral_foundations = [\"Care\", \"Fairness\", \"Loyalty\", \"Authority\", \"Sanctity\", \"Liberty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid switch - \"huggingface\".\n"
     ]
    }
   ],
   "source": [
    "ls .cache/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m label_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m zero_samples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweet Text\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 10\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoral_foundations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Count labels that meet the threshold\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     selected_labels \u001b[38;5;241m=\u001b[39m [label \u001b[38;5;28;01mfor\u001b[39;00m label, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m], result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold]\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[1;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1294\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    228\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[0;32m    231\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[0;32m    236\u001b[0m }\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1763\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing input embeddings is currently not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[1;32m-> 1763\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1778\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# last hidden state\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m eos_mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id)\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1528\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1521\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1522\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1523\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1524\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1525\u001b[0m     )\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1528\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1380\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1368\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1369\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         use_cache,\n\u001b[0;32m   1378\u001b[0m     )\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1380\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1393\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:666\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    664\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    674\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:247\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    244\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    245\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[1;32m--> 247\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,):\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Zero shot classification \n",
    "#checking the thresholds\n",
    "\n",
    "\n",
    "# Select threshold for final multi-label classification\n",
    "final_threshold = 0.3\n",
    "all_predicted_labels = []\n",
    "\n",
    "for text in zero_shot_sample[\"text\"]:\n",
    "    result = classifier(text, candidate_labels)\n",
    "    \n",
    "    # Append selected labels\n",
    "    selected_labels = [label for label, score in zip(result[\"labels\"], result[\"scores\"]) if score >= final_threshold]\n",
    "    all_predicted_labels.extend(selected_labels)\n",
    "\n",
    "# Count label occurrences\n",
    "label_counts = Counter(all_predicted_labels)\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(label_counts.keys(), label_counts.values(), color='skyblue')\n",
    "plt.title(\"Class Distribution after Zero-Shot Multi-Label Classification\")\n",
    "plt.xlabel(\"Moral Foundations\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot label counts vs threshold\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, label_counts, marker='o')\n",
    "plt.title(\"Threshold Tuning for Multi-Label Classification\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Total Assigned Labels\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Helping the poor is a moral duty.', 'labels': ['Care', 'Liberty', 'Sanctity', 'Fairness', 'Authority', 'Loyalty'], 'scores': [0.5870230793952942, 0.09679284691810608, 0.0940442606806755, 0.08719964325428009, 0.0774305984377861, 0.057509563863277435]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"Helping the poor is a moral duty.\"\n",
    "candidate_labels = [\"Care\", \"Fairness\", \"Loyalty\", \"Authority\", \"Sanctity\", \"Liberty\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(text, candidate_labels)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
